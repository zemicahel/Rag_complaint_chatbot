{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca76f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Dataset Shape: (15000, 20)\n",
      "Product Distribution in Sample:\n",
      " Product\n",
      "Checking or savings account                           0.328733\n",
      "Credit card or prepaid card                           0.254600\n",
      "Money transfer, virtual currency, or money service    0.227667\n",
      "Credit card                                           0.189000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned data from Task 1\n",
    "df = pd.read_csv('../data/processed/filtered_complaints.csv')\n",
    "\n",
    "# Create a stratified sample (15,000 records)\n",
    "# This ensures \"Money Transfers\" (smaller category) is still well-represented \n",
    "# relative to \"Credit Cards\" (larger category)\n",
    "sample_size = 15000\n",
    "df_sample, _ = train_test_split(\n",
    "    df, \n",
    "    train_size=sample_size, \n",
    "    stratify=df['Product'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Sampled Dataset Shape: {df_sample.shape}\")\n",
    "print(\"Product Distribution in Sample:\\n\", df_sample['Product'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ba10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 41457 total chunks from 15000 complaints.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We choose 500 chars (~100 words) with a 50 char overlap\n",
    "# Overlap ensures that context isn't lost if a sentence is cut in half\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# Function to process the dataframe into chunks with metadata\n",
    "def create_chunks(df):\n",
    "    chunked_docs = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Create chunks for this specific narrative\n",
    "        chunks = text_splitter.split_text(row['cleaned_narrative'])\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Store the text and the metadata needed to trace it back\n",
    "            chunked_docs.append({\n",
    "                \"page_content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"complaint_id\": row['Complaint ID'],\n",
    "                    \"product\": row['Product'],\n",
    "                    \"company\": row['Company'],\n",
    "                    \"chunk_index\": i\n",
    "                }\n",
    "            })\n",
    "    return chunked_docs\n",
    "\n",
    "docs = create_chunks(df_sample)\n",
    "print(f\"Created {len(docs)} total chunks from {len(df_sample)} complaints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5448db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/kaim/Rag_complaint_chatbot/.venv/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:68\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;66;03m# Change to 'cuda' if using a GPU\u001b[39;00m\n\u001b[1;32m      7\u001b[0m encode_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Create and Persist the Vector Store\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# This will save the database into the 'vector_store/' folder\u001b[39;00m\n\u001b[1;32m     17\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_store/complaints_db\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Documents/kaim/Rag_complaint_chatbot/.venv/lib/python3.10/site-packages/langchain_huggingface/embeddings/huggingface.py:74\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     70\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_optimum_intel_available() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ipex_available():\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 1. Initialize the Embedding Model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'} # Change to 'cuda' if using a GPU\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# 2. Create and Persist the Vector Store\n",
    "# This will save the database into the 'vector_store/' folder\n",
    "persist_directory = '../vector_store/complaints_db'\n",
    "\n",
    "# Extract text and metadata for Chroma\n",
    "texts = [doc['page_content'] for doc in docs]\n",
    "metadatas = [doc['metadata'] for doc in docs]\n",
    "\n",
    "print(\"Starting Indexing... this may take 5-10 minutes depending on CPU.\")\n",
    "\n",
    "vector_db = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"Vector Store saved at {persist_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
